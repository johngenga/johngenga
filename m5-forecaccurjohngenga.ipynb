{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18599,"databundleVersionId":1236839,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:20:49.244907Z","iopub.execute_input":"2024-08-12T17:20:49.245264Z","iopub.status.idle":"2024-08-12T17:20:50.193733Z","shell.execute_reply.started":"2024-08-12T17:20:49.245231Z","shell.execute_reply":"2024-08-12T17:20:50.192338Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/m5-forecasting-accuracy/calendar.csv\n/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\n/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\n/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\n/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Define the common path to the files\ndata_path = '/kaggle/input/m5-forecasting-accuracy/'\n\n# Load datasets using the common path\nsales_train_validation = pd.read_csv(f'{data_path}sales_train_validation.csv')\nsales_train_evaluation = pd.read_csv(f'{data_path}sales_train_evaluation.csv')\ncalendar = pd.read_csv(f'{data_path}calendar.csv')\nsell_prices = pd.read_csv(f'{data_path}sell_prices.csv')\nsample_submission = pd.read_csv(f'{data_path}sample_submission.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:20:50.199350Z","iopub.execute_input":"2024-08-12T17:20:50.199727Z","iopub.status.idle":"2024-08-12T17:21:07.497358Z","shell.execute_reply.started":"2024-08-12T17:20:50.199676Z","shell.execute_reply":"2024-08-12T17:21:07.496235Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Melt the sales_train_validation data\nsales_train_validation_melted = sales_train_validation.melt(\n    id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n    var_name='d',\n    value_name='sales'\n)\n\n# Display the first few rows of the melted DataFrame\nprint(\"Melted Sales Train Validation Data:\")\nprint(sales_train_validation_melted.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:21:07.498829Z","iopub.execute_input":"2024-08-12T17:21:07.499246Z","iopub.status.idle":"2024-08-12T17:21:24.617683Z","shell.execute_reply.started":"2024-08-12T17:21:07.499207Z","shell.execute_reply":"2024-08-12T17:21:24.616600Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Melted Sales Train Validation Data:\n                              id        item_id    dept_id   cat_id store_id  \\\n0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n\n  state_id    d  sales  \n0       CA  d_1      0  \n1       CA  d_1      0  \n2       CA  d_1      0  \n3       CA  d_1      0  \n4       CA  d_1      0  \n","output_type":"stream"}]},{"cell_type":"code","source":"def merge_in_chunks(df, calendar, prices, chunk_size=1500000):\n    # List to hold the merged chunks\n    merged_chunks = []\n    \n    # Number of chunks\n    num_chunks = len(df) // chunk_size + 1\n    \n    for i in range(num_chunks):\n        start = i * chunk_size\n        end = (i + 1) * chunk_size\n        chunk = df[start:end]\n        chunk = chunk.merge(calendar, on='d', how='left')\n        chunk = chunk.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n              \n        merged_chunks.append(chunk)\n        \n        print(f\"Chunk {i+1}/{num_chunks} merged\")\n    \n    # Concatenate all chunks\n    merged_df = pd.concat(merged_chunks, ignore_index=True)\n    \n    return merged_df\n\n# Merge the melted DataFrame in chunks\nsales_train_validation_merged = merge_in_chunks(sales_train_validation_melted, calendar, sell_prices)\n\n# Display the first few rows of the merged DataFrame\nprint(\"Merged Sales Train Validation Data:\")\nprint(sales_train_validation_merged.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:21:24.621207Z","iopub.execute_input":"2024-08-12T17:21:24.621644Z","iopub.status.idle":"2024-08-12T17:24:53.697037Z","shell.execute_reply.started":"2024-08-12T17:21:24.621611Z","shell.execute_reply":"2024-08-12T17:24:53.695689Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Chunk 1/39 merged\nChunk 2/39 merged\nChunk 3/39 merged\nChunk 4/39 merged\nChunk 5/39 merged\nChunk 6/39 merged\nChunk 7/39 merged\nChunk 8/39 merged\nChunk 9/39 merged\nChunk 10/39 merged\nChunk 11/39 merged\nChunk 12/39 merged\nChunk 13/39 merged\nChunk 14/39 merged\nChunk 15/39 merged\nChunk 16/39 merged\nChunk 17/39 merged\nChunk 18/39 merged\nChunk 19/39 merged\nChunk 20/39 merged\nChunk 21/39 merged\nChunk 22/39 merged\nChunk 23/39 merged\nChunk 24/39 merged\nChunk 25/39 merged\nChunk 26/39 merged\nChunk 27/39 merged\nChunk 28/39 merged\nChunk 29/39 merged\nChunk 30/39 merged\nChunk 31/39 merged\nChunk 32/39 merged\nChunk 33/39 merged\nChunk 34/39 merged\nChunk 35/39 merged\nChunk 36/39 merged\nChunk 37/39 merged\nChunk 38/39 merged\nChunk 39/39 merged\nMerged Sales Train Validation Data:\n                              id        item_id    dept_id   cat_id store_id  \\\n0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n\n  state_id    d  sales        date  wm_yr_wk  ... month  year  event_name_1  \\\n0       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n1       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n2       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n3       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n4       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n\n   event_type_1 event_name_2 event_type_2 snap_CA snap_TX  snap_WI  sell_price  \n0           NaN          NaN          NaN       0       0        0         NaN  \n1           NaN          NaN          NaN       0       0        0         NaN  \n2           NaN          NaN          NaN       0       0        0         NaN  \n3           NaN          NaN          NaN       0       0        0         NaN  \n4           NaN          NaN          NaN       0       0        0         NaN  \n\n[5 rows x 22 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fill missing sell prices with the last available price\nsales_train_validation_merged['sell_price'] = sales_train_validation_merged['sell_price'].ffill()\n\ndef create_rolling_features(df, window_sizes):\n    for window in window_sizes:\n        df[f'rolling_mean_{window}'] = df.groupby(['id'])['sales'].shift(1).rolling(window=window).mean()\n        df[f'rolling_std_{window}'] = df.groupby(['id'])['sales'].shift(1).rolling(window=window).std()\n    return df\n\n# Apply rolling features\nwindow_sizes = [7, 30]\nsales_train_validation_merged = create_rolling_features(sales_train_validation_merged, window_sizes)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:24:53.698731Z","iopub.execute_input":"2024-08-12T17:24:53.699071Z","iopub.status.idle":"2024-08-12T17:25:53.092999Z","shell.execute_reply.started":"2024-08-12T17:24:53.699040Z","shell.execute_reply":"2024-08-12T17:25:53.091388Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Handle NaNs in rolling features\nrolling_mean_cols = [f'rolling_mean_{window}' for window in window_sizes]\nrolling_std_cols = [f'rolling_std_{window}' for window in window_sizes]\n\n# Fill NaNs in rolling features with the overall mean and std (assuming that initial periods can use overall stats)\nsales_train_validation_merged[rolling_mean_cols] = sales_train_validation_merged[rolling_mean_cols].fillna(sales_train_validation_merged['sales'].mean())\nsales_train_validation_merged[rolling_std_cols] = sales_train_validation_merged[rolling_std_cols].fillna(sales_train_validation_merged['sales'].std())","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:25:53.094513Z","iopub.execute_input":"2024-08-12T17:25:53.094854Z","iopub.status.idle":"2024-08-12T17:25:56.326512Z","shell.execute_reply.started":"2024-08-12T17:25:53.094825Z","shell.execute_reply":"2024-08-12T17:25:56.325557Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Reduce Memory Usage by Downcasting\nHere we convert data types to use less memory. For example, converting float64 to float32 and int64 to int32 can save memory.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef reduce_memory_usage(df):\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # Check if the column is a date and skip if it is\n        if np.issubdtype(col_type, np.datetime64):\n            continue\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n        else:\n            df[col] = df[col].astype('category')\n\n    return df\n\nsales_train_validation_merged = reduce_memory_usage(sales_train_validation_merged)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:25:56.327913Z","iopub.execute_input":"2024-08-12T17:25:56.328237Z","iopub.status.idle":"2024-08-12T17:27:01.994960Z","shell.execute_reply.started":"2024-08-12T17:25:56.328209Z","shell.execute_reply":"2024-08-12T17:27:01.993796Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Convert to string first, then to datetime\nsales_train_validation_merged['date'] = sales_train_validation_merged['date'].astype(str)\nsales_train_validation_merged['date'] = pd.to_datetime(sales_train_validation_merged['date'], errors='coerce')\n\n# Check the result\nprint(sales_train_validation_merged['date'].dtype)\nprint(sales_train_validation_merged['date'].head(10))","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:27:01.996318Z","iopub.execute_input":"2024-08-12T17:27:01.996670Z","iopub.status.idle":"2024-08-12T17:27:26.353308Z","shell.execute_reply.started":"2024-08-12T17:27:01.996640Z","shell.execute_reply":"2024-08-12T17:27:26.351980Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"datetime64[ns]\n0   2011-01-29\n1   2011-01-29\n2   2011-01-29\n3   2011-01-29\n4   2011-01-29\n5   2011-01-29\n6   2011-01-29\n7   2011-01-29\n8   2011-01-29\n9   2011-01-29\nName: date, dtype: datetime64[ns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# List of columns to remove\ncolumns_to_remove = [\n    'weekday', 'wday', 'month', 'year', 'event_type_1', 'event_name_2', 'event_type_2'\n]\nRemove Unnecessary Columns\nDrop any columns that aren't essential for the modeling process.","metadata":{}},{"cell_type":"code","source":"# List of columns to remove\ncolumns_to_remove = [\n    'dept_id', 'cat_id', 'state_id', 'weekday', \n    'wday', 'month', 'year', 'event_type_1', \n    'event_name_2', 'event_type_2'\n]\n\n# Remove the unwanted columns\nsales_train_validation_merged = sales_train_validation_merged.drop(columns=columns_to_remove)\n\n# Binary encoding: 1 if there's any event, 0 otherwise\nsales_train_validation_merged['event_occurred'] = sales_train_validation_merged['event_name_1'].notna().astype(int)\n\n# Drop the original 'event_name_1' column\nsales_train_validation_merged = sales_train_validation_merged.drop('event_name_1', axis=1)\n\n# Verify the final DataFrame\nprint(sales_train_validation_merged.head())\nprint(sales_train_validation_merged.columns)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:27:26.354551Z","iopub.execute_input":"2024-08-12T17:27:26.354858Z","iopub.status.idle":"2024-08-12T17:27:33.132981Z","shell.execute_reply.started":"2024-08-12T17:27:26.354832Z","shell.execute_reply":"2024-08-12T17:27:33.131908Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"                              id        item_id store_id    d  sales  \\\n0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001     CA_1  d_1      0   \n1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002     CA_1  d_1      0   \n2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003     CA_1  d_1      0   \n3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004     CA_1  d_1      0   \n4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005     CA_1  d_1      0   \n\n        date  wm_yr_wk  snap_CA  snap_TX  snap_WI  sell_price  rolling_mean_7  \\\n0 2011-01-29     11101        0        0        0         NaN        1.125977   \n1 2011-01-29     11101        0        0        0         NaN        1.125977   \n2 2011-01-29     11101        0        0        0         NaN        1.125977   \n3 2011-01-29     11101        0        0        0         NaN        1.125977   \n4 2011-01-29     11101        0        0        0         NaN        1.125977   \n\n   rolling_std_7  rolling_mean_30  rolling_std_30  event_occurred  \n0       3.873047         1.125977        3.873047               0  \n1       3.873047         1.125977        3.873047               0  \n2       3.873047         1.125977        3.873047               0  \n3       3.873047         1.125977        3.873047               0  \n4       3.873047         1.125977        3.873047               0  \nIndex(['id', 'item_id', 'store_id', 'd', 'sales', 'date', 'wm_yr_wk',\n       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'rolling_mean_7',\n       'rolling_std_7', 'rolling_mean_30', 'rolling_std_30', 'event_occurred'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the features and target\nfeatures = ['sell_price', 'wm_yr_wk', 'snap_CA', 'snap_TX', 'snap_WI','rolling_mean_7',\n            'rolling_mean_30', 'rolling_std_7', 'rolling_std_30', 'event_occurred']\ntarget = 'sales'","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:27:33.134254Z","iopub.execute_input":"2024-08-12T17:27:33.134583Z","iopub.status.idle":"2024-08-12T17:27:33.139655Z","shell.execute_reply.started":"2024-08-12T17:27:33.134554Z","shell.execute_reply":"2024-08-12T17:27:33.138479Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"models = {}  # Dictionary to store models and scalers for each department\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Get unique ids\nid = sales_train_validation_merged['id'].unique()\n\n# Calculate total number of items\ntotal_items = len(id)\n\nfor idx, items in enumerate(id, start=1):\n    # Filter data for the current department\n    items_data = sales_train_validation_merged[sales_train_validation_merged['id'] == items]\n    \n    # Prepare features and target\n    X = items_data[features]\n    y = items_data[target]\n    \n    # Split data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Standardize the features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Handle NaNs\n    X_train_scaled = pd.DataFrame(X_train_scaled, columns=features)\n    X_train_scaled.fillna(X_train_scaled.mean(), inplace=True)\n    X_val_scaled = pd.DataFrame(X_val_scaled, columns=features)\n    X_val_scaled.fillna(X_val_scaled.mean(), inplace=True)\n    X_train_scaled = X_train_scaled.values\n    X_val_scaled = X_val_scaled.values\n    \n    # Initialize the model\n    model = SGDRegressor(max_iter=1000, tol=1e-3)\n    \n    # Train the model on the entire dataset\n    model.fit(X_train_scaled, y_train)\n    \n    # Store the model and scaler for the current department\n    models[items] = (model, scaler)\n    \n    # Evaluate the model\n    y_pred = model.predict(X_val_scaled)\n    mse = mean_squared_error(y_val, y_pred)\n    \n    # Print progress every 5,000 items\n    if idx % 5000 == 0 or idx == total_items:\n        progress = (idx / total_items) * 100\n        print(f\"Progress: {progress:.2f}% ({idx} /{total_items} items processed)\")\n\nprint(\"All items processed and models stored.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:27:33.140996Z","iopub.execute_input":"2024-08-12T17:27:33.141299Z","iopub.status.idle":"2024-08-12T17:55:52.035979Z","shell.execute_reply.started":"2024-08-12T17:27:33.141273Z","shell.execute_reply":"2024-08-12T17:55:52.034800Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Progress: 16.40% (5000 /30490 items processed)\nProgress: 32.80% (10000 /30490 items processed)\nProgress: 49.20% (15000 /30490 items processed)\nProgress: 65.60% (20000 /30490 items processed)\nProgress: 81.99% (25000 /30490 items processed)\nProgress: 98.39% (30000 /30490 items processed)\nProgress: 100.00% (30490 /30490 items processed)\nAll items processed and models stored.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Melt the sample submission data frame\nsample_submission = pd.read_csv(f'{data_path}sample_submission.csv')\n\nsample_submission_melted = sample_submission.melt(\n    id_vars=['id'],\n    value_vars=['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n       'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28'],\n    value_name='sales')\nprint(sample_submission_melted.dtypes)\nprint(sample_submission_melted.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T20:56:16.213451Z","iopub.execute_input":"2024-08-12T20:56:16.214548Z","iopub.status.idle":"2024-08-12T20:56:16.525089Z","shell.execute_reply.started":"2024-08-12T20:56:16.214508Z","shell.execute_reply":"2024-08-12T20:56:16.523761Z"},"trusted":true},"execution_count":101,"outputs":[{"name":"stdout","text":"id          object\nvariable    object\nsales        int64\ndtype: object\n(1707440, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the mapping of variables to dates\nvariable_to_date = {\n    'F1': '2016-04-25',\n    'F2': '2016-04-26',\n    'F3': '2016-04-27',\n    'F4': '2016-04-28',\n    'F5': '2016-04-29',\n    'F6': '2016-04-30',\n    'F7': '2016-05-1',\n    'F8': '2016-05-2',\n    'F9': '2016-05-3',\n    'F10': '2016-05-4',\n    'F11': '2016-05-5',\n    'F12': '2016-05-6',\n    'F13': '2016-05-7',\n    'F14': '2016-05-8',\n    'F15': '2016-05-9',\n    'F16': '2016-05-10',\n    'F17': '2016-05-11',\n    'F18': '2016-05-12',\n    'F19': '2016-05-13',\n    'F20': '2016-05-14',\n    'F21': '2016-05-15',\n    'F22': '2016-05-16',\n    'F23': '2016-05-17',\n    'F24': '2016-05-18',\n    'F25': '2016-05-19',\n    'F26': '2016-05-20',\n    'F27': '2016-05-21',\n    'F28': '2016-05-22',  \n   }\n# Add a new column for dates\nsample_submission_melted['date'] = sample_submission_melted['variable'].map(variable_to_date)\nsample_submission_melted['date'] = pd.to_datetime(sample_submission_melted['date'])\n\nprint(\"\\nDataFrame with Dates:\")\nprint(sample_submission_melted.dtypes)\nprint(sample_submission_melted.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T20:56:21.503781Z","iopub.execute_input":"2024-08-12T20:56:21.504219Z","iopub.status.idle":"2024-08-12T20:56:21.812780Z","shell.execute_reply.started":"2024-08-12T20:56:21.504187Z","shell.execute_reply":"2024-08-12T20:56:21.811501Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stdout","text":"\nDataFrame with Dates:\nid                  object\nvariable            object\nsales                int64\ndate        datetime64[ns]\ndtype: object\n(1707440, 4)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Merge the sample_submission_melted with calendar to get relevant features.\ncalendar = pd.read_csv(f'{data_path}calendar.csv')\ncalendar['date'] = pd.to_datetime(calendar['date'])\nsample_submission_melted = pd.merge(sample_submission_melted, calendar, on='date', how='left')","metadata":{"execution":{"iopub.status.busy":"2024-08-12T20:56:28.326803Z","iopub.execute_input":"2024-08-12T20:56:28.327505Z","iopub.status.idle":"2024-08-12T20:56:28.807197Z","shell.execute_reply.started":"2024-08-12T20:56:28.327471Z","shell.execute_reply":"2024-08-12T20:56:28.806053Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"print(sample_submission_melted.head)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T20:56:34.265152Z","iopub.execute_input":"2024-08-12T20:56:34.265950Z","iopub.status.idle":"2024-08-12T20:56:34.277891Z","shell.execute_reply.started":"2024-08-12T20:56:34.265913Z","shell.execute_reply":"2024-08-12T20:56:34.276728Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"<bound method NDFrame.head of                                     id variable  sales       date  wm_yr_wk  \\\n0        HOBBIES_1_001_CA_1_validation       F1      0 2016-04-25     11613   \n1        HOBBIES_1_002_CA_1_validation       F1      0 2016-04-25     11613   \n2        HOBBIES_1_003_CA_1_validation       F1      0 2016-04-25     11613   \n3        HOBBIES_1_004_CA_1_validation       F1      0 2016-04-25     11613   \n4        HOBBIES_1_005_CA_1_validation       F1      0 2016-04-25     11613   \n...                                ...      ...    ...        ...       ...   \n1707435    FOODS_3_823_WI_3_evaluation      F28      0 2016-05-22     11617   \n1707436    FOODS_3_824_WI_3_evaluation      F28      0 2016-05-22     11617   \n1707437    FOODS_3_825_WI_3_evaluation      F28      0 2016-05-22     11617   \n1707438    FOODS_3_826_WI_3_evaluation      F28      0 2016-05-22     11617   \n1707439    FOODS_3_827_WI_3_evaluation      F28      0 2016-05-22     11617   \n\n        weekday  wday  month  year       d event_name_1 event_type_1  \\\n0        Monday     3      4  2016  d_1914          NaN          NaN   \n1        Monday     3      4  2016  d_1914          NaN          NaN   \n2        Monday     3      4  2016  d_1914          NaN          NaN   \n3        Monday     3      4  2016  d_1914          NaN          NaN   \n4        Monday     3      4  2016  d_1914          NaN          NaN   \n...         ...   ...    ...   ...     ...          ...          ...   \n1707435  Sunday     2      5  2016  d_1941          NaN          NaN   \n1707436  Sunday     2      5  2016  d_1941          NaN          NaN   \n1707437  Sunday     2      5  2016  d_1941          NaN          NaN   \n1707438  Sunday     2      5  2016  d_1941          NaN          NaN   \n1707439  Sunday     2      5  2016  d_1941          NaN          NaN   \n\n        event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \n0                NaN          NaN        0        0        0  \n1                NaN          NaN        0        0        0  \n2                NaN          NaN        0        0        0  \n3                NaN          NaN        0        0        0  \n4                NaN          NaN        0        0        0  \n...              ...          ...      ...      ...      ...  \n1707435          NaN          NaN        0        0        0  \n1707436          NaN          NaN        0        0        0  \n1707437          NaN          NaN        0        0        0  \n1707438          NaN          NaN        0        0        0  \n1707439          NaN          NaN        0        0        0  \n\n[1707440 rows x 17 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Merge the sample_submission_melted with sell prices to get relevant features. \n# We loop through the sales_validation_document.\n\n# Prepare the sales_train_validation_merged.\nsales_train_validation = pd.read_csv(f'{data_path}sales_train_validation.csv')\n\n# Select only the 'id' and 'item_id' columns\nid_subset_df = sales_train_validation[['id', 'item_id','store_id']].drop_duplicates()\nid_subset_df['id'] = id_subset_df['id'].astype('category')\nid_subset_df['item_id'] = id_subset_df['item_id'].astype('category')\nid_subset_df['store_id'] = id_subset_df['store_id'].astype('category')\nsample_submission_melted['id'] = sample_submission_melted['id'].astype('category')\n\nsample_submission_melted = pd.merge(\n    sample_submission_melted,     # Original dataframe\n    id_subset_df,                         # Dataframe containing 'sell_price'\n    on='id',  # Columns to match on\n    how='left'                           # Use 'left' to maintain all rows from the original dataframe\n)\n\nprint(sample_submission_melted.shape)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T20:56:51.326811Z","iopub.execute_input":"2024-08-12T20:56:51.327200Z","iopub.status.idle":"2024-08-12T20:56:57.842757Z","shell.execute_reply.started":"2024-08-12T20:56:51.327171Z","shell.execute_reply":"2024-08-12T20:56:57.841617Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stdout","text":"(1707440, 19)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Merge 'sample_submission_melted_merged' with 'sell_prices' based on 'store_id', 'item_id', and 'wm_yr_wk'\nsample_submission_melted = pd.merge(\n    sample_submission_melted,     # Original dataframe\n    sell_prices,                         # Dataframe containing 'sell_price'\n    on=['store_id', 'item_id', 'wm_yr_wk'],  # Columns to match on\n    how='left'                           # Use 'left' to maintain all rows from the original dataframe\n)\n\n# Verify the merge\nprint(sample_submission_melted.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-12T20:57:05.632730Z","iopub.execute_input":"2024-08-12T20:57:05.633603Z","iopub.status.idle":"2024-08-12T20:57:10.495881Z","shell.execute_reply.started":"2024-08-12T20:57:05.633561Z","shell.execute_reply":"2024-08-12T20:57:10.494777Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stdout","text":"                              id variable  sales       date  wm_yr_wk weekday  \\\n0  HOBBIES_1_001_CA_1_validation       F1      0 2016-04-25     11613  Monday   \n1  HOBBIES_1_002_CA_1_validation       F1      0 2016-04-25     11613  Monday   \n2  HOBBIES_1_003_CA_1_validation       F1      0 2016-04-25     11613  Monday   \n3  HOBBIES_1_004_CA_1_validation       F1      0 2016-04-25     11613  Monday   \n4  HOBBIES_1_005_CA_1_validation       F1      0 2016-04-25     11613  Monday   \n\n   wday  month  year       d event_name_1 event_type_1 event_name_2  \\\n0     3      4  2016  d_1914          NaN          NaN          NaN   \n1     3      4  2016  d_1914          NaN          NaN          NaN   \n2     3      4  2016  d_1914          NaN          NaN          NaN   \n3     3      4  2016  d_1914          NaN          NaN          NaN   \n4     3      4  2016  d_1914          NaN          NaN          NaN   \n\n  event_type_2  snap_CA  snap_TX  snap_WI        item_id store_id  sell_price  \n0          NaN        0        0        0  HOBBIES_1_001     CA_1        8.38  \n1          NaN        0        0        0  HOBBIES_1_002     CA_1        3.97  \n2          NaN        0        0        0  HOBBIES_1_003     CA_1        2.97  \n3          NaN        0        0        0  HOBBIES_1_004     CA_1        4.64  \n4          NaN        0        0        0  HOBBIES_1_005     CA_1        2.88  \n","output_type":"stream"}]},{"cell_type":"code","source":"submission_merged = sample_submission_melted\nprint(submission_merged.head)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T20:57:33.473794Z","iopub.execute_input":"2024-08-12T20:57:33.474819Z","iopub.status.idle":"2024-08-12T20:57:33.582085Z","shell.execute_reply.started":"2024-08-12T20:57:33.474780Z","shell.execute_reply":"2024-08-12T20:57:33.580718Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stdout","text":"<bound method NDFrame.head of                                     id variable  sales       date  wm_yr_wk  \\\n0        HOBBIES_1_001_CA_1_validation       F1      0 2016-04-25     11613   \n1        HOBBIES_1_002_CA_1_validation       F1      0 2016-04-25     11613   \n2        HOBBIES_1_003_CA_1_validation       F1      0 2016-04-25     11613   \n3        HOBBIES_1_004_CA_1_validation       F1      0 2016-04-25     11613   \n4        HOBBIES_1_005_CA_1_validation       F1      0 2016-04-25     11613   \n...                                ...      ...    ...        ...       ...   \n1707435    FOODS_3_823_WI_3_evaluation      F28      0 2016-05-22     11617   \n1707436    FOODS_3_824_WI_3_evaluation      F28      0 2016-05-22     11617   \n1707437    FOODS_3_825_WI_3_evaluation      F28      0 2016-05-22     11617   \n1707438    FOODS_3_826_WI_3_evaluation      F28      0 2016-05-22     11617   \n1707439    FOODS_3_827_WI_3_evaluation      F28      0 2016-05-22     11617   \n\n        weekday  wday  month  year       d event_name_1 event_type_1  \\\n0        Monday     3      4  2016  d_1914          NaN          NaN   \n1        Monday     3      4  2016  d_1914          NaN          NaN   \n2        Monday     3      4  2016  d_1914          NaN          NaN   \n3        Monday     3      4  2016  d_1914          NaN          NaN   \n4        Monday     3      4  2016  d_1914          NaN          NaN   \n...         ...   ...    ...   ...     ...          ...          ...   \n1707435  Sunday     2      5  2016  d_1941          NaN          NaN   \n1707436  Sunday     2      5  2016  d_1941          NaN          NaN   \n1707437  Sunday     2      5  2016  d_1941          NaN          NaN   \n1707438  Sunday     2      5  2016  d_1941          NaN          NaN   \n1707439  Sunday     2      5  2016  d_1941          NaN          NaN   \n\n        event_name_2 event_type_2  snap_CA  snap_TX  snap_WI        item_id  \\\n0                NaN          NaN        0        0        0  HOBBIES_1_001   \n1                NaN          NaN        0        0        0  HOBBIES_1_002   \n2                NaN          NaN        0        0        0  HOBBIES_1_003   \n3                NaN          NaN        0        0        0  HOBBIES_1_004   \n4                NaN          NaN        0        0        0  HOBBIES_1_005   \n...              ...          ...      ...      ...      ...            ...   \n1707435          NaN          NaN        0        0        0            NaN   \n1707436          NaN          NaN        0        0        0            NaN   \n1707437          NaN          NaN        0        0        0            NaN   \n1707438          NaN          NaN        0        0        0            NaN   \n1707439          NaN          NaN        0        0        0            NaN   \n\n        store_id  sell_price  \n0           CA_1        8.38  \n1           CA_1        3.97  \n2           CA_1        2.97  \n3           CA_1        4.64  \n4           CA_1        2.88  \n...          ...         ...  \n1707435      NaN         NaN  \n1707436      NaN         NaN  \n1707437      NaN         NaN  \n1707438      NaN         NaN  \n1707439      NaN         NaN  \n\n[1707440 rows x 20 columns]>\n","output_type":"stream"}]},{"cell_type":"code","source":"# List of columns to remove\ncolumns_to_remove = [\n    'weekday', 'wday', 'month', 'year', 'event_type_1', 'event_name_2', 'event_type_2'\n]\n# Remove the unwanted columns\nsubmission_merged = submission_merged.drop(columns=columns_to_remove)\n\n# Binary encoding: 1 if there's any event, 0 otherwise\nsubmission_merged['event_occurred'] = submission_merged['event_name_1'].notna().astype(int)\n\n# Drop the original 'event_name_1' column\nsubmission_merged = submission_merged.drop('event_name_1', axis=1)\n\n# Select relevant columns from sales_train_validation_merged\nrolling_features = sales_train_validation_merged[['id', 'date', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_30', 'rolling_std_30']]\n\n# Merge rolling features into next_28_days based on 'id' and 'date'\nsubmission_merged = submission_merged.merge(rolling_features, how='left', on=['id', 'date'])\n\n# Align data types in next_28_days with those in X_train\nsubmission_merged['sell_price'] = submission_merged['sell_price'].astype('float16')\nsubmission_merged['wm_yr_wk'] = submission_merged['wm_yr_wk'].astype('int16')\nsubmission_merged['snap_CA'] = submission_merged['snap_CA'].astype('int8')\nsubmission_merged['snap_TX'] = submission_merged['snap_TX'].astype('int8')\nsubmission_merged['snap_WI'] = submission_merged['snap_WI'].astype('int8')\n\n# Fill missing values in rolling features\nsubmission_merged[['rolling_mean_7', 'rolling_std_7', 'rolling_mean_30', 'rolling_std_30']] = submission_merged[['rolling_mean_7', 'rolling_std_7', 'rolling_mean_30', 'rolling_std_30']].fillna(0)\n\n# Verify the merge\nprint(submission_merged.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-12T20:58:56.169098Z","iopub.execute_input":"2024-08-12T20:58:56.169574Z","iopub.status.idle":"2024-08-12T20:59:34.946374Z","shell.execute_reply.started":"2024-08-12T20:58:56.169541Z","shell.execute_reply":"2024-08-12T20:59:34.945286Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stdout","text":"                              id variable  sales       date  wm_yr_wk       d  \\\n0  HOBBIES_1_001_CA_1_validation       F1      0 2016-04-25     11613  d_1914   \n1  HOBBIES_1_002_CA_1_validation       F1      0 2016-04-25     11613  d_1914   \n2  HOBBIES_1_003_CA_1_validation       F1      0 2016-04-25     11613  d_1914   \n3  HOBBIES_1_004_CA_1_validation       F1      0 2016-04-25     11613  d_1914   \n4  HOBBIES_1_005_CA_1_validation       F1      0 2016-04-25     11613  d_1914   \n\n   snap_CA  snap_TX  snap_WI        item_id store_id  sell_price  \\\n0        0        0        0  HOBBIES_1_001     CA_1    8.382812   \n1        0        0        0  HOBBIES_1_002     CA_1    3.970703   \n2        0        0        0  HOBBIES_1_003     CA_1    2.970703   \n3        0        0        0  HOBBIES_1_004     CA_1    4.640625   \n4        0        0        0  HOBBIES_1_005     CA_1    2.880859   \n\n   event_occurred  rolling_mean_7  rolling_std_7  rolling_mean_30  \\\n0               0             0.0            0.0              0.0   \n1               0             0.0            0.0              0.0   \n2               0             0.0            0.0              0.0   \n3               0             0.0            0.0              0.0   \n4               0             0.0            0.0              0.0   \n\n   rolling_std_30  \n0             0.0  \n1             0.0  \n2             0.0  \n3             0.0  \n4             0.0  \n","output_type":"stream"}]},{"cell_type":"code","source":"next_28_days = submission_merged\n\nimport numpy as np\nimport pandas as pd\n\n# Assuming features used in the model\nfeatures = ['sell_price', 'wm_yr_wk', 'snap_CA', 'snap_TX', 'snap_WI', \n            'rolling_mean_7', 'rolling_mean_30', 'rolling_std_7', 'rolling_std_30', 'event_occurred']\n\n# Initialize an empty dictionary to store predictions\npredictions_dict = {}\n\n# Loop through each unique item id in next_28_days data\nfor idx, items in enumerate(next_28_days['id'].unique(), start=1):\n    \n    # Filter data for the current item\n    items_data = next_28_days[next_28_days['id'] == items]\n    \n    # Extract the features\n    X_test = items_data[features]\n    \n    # Load the model and scaler for the current item\n    models[items] = (model, scaler)\n    \n    if model and scaler:\n        # Standardize the features using the scaler\n        X_test_scaled = scaler.transform(X_test)\n        \n        # Handle NaNs (if any) by filling them with the mean of the column\n        X_test_scaled = pd.DataFrame(X_test_scaled, columns=features)\n        X_test_scaled.fillna(X_test_scaled.mean(), inplace=True)\n        X_test_scaled = X_test_scaled.values\n        \n        # Make predictions for the current item\n        predictions = model.predict(X_test_scaled)\n        \n        # Store predictions in a dictionary\n        predictions_dict[items] = predictions\n    \n    # Print progress every 5,000 items\n    if idx % 5000 == 0 or idx == len(next_28_days['id'].unique()):\n        progress = (idx / len(next_28_days['id'].unique())) * 100\n        print(f\"Progress: {progress:.2f}% ({idx} / {len(next_28_days['id'].unique())} items processed)\")\n\nprint(\"All predictions generated.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-12T21:30:56.979526Z","iopub.execute_input":"2024-08-12T21:30:56.979948Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Progress: 8.20% (5000 / 60980 items processed)\nProgress: 16.40% (10000 / 60980 items processed)\n","output_type":"stream"}]}]}