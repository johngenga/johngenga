{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18599,"databundleVersionId":1236839,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:33:59.495019Z","iopub.execute_input":"2024-08-10T19:33:59.495475Z","iopub.status.idle":"2024-08-10T19:33:59.922541Z","shell.execute_reply.started":"2024-08-10T19:33:59.495437Z","shell.execute_reply":"2024-08-10T19:33:59.921553Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/m5-forecasting-accuracy/calendar.csv\n/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\n/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\n/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\n/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n# Define the common path to the files\ndata_path = '/kaggle/input/m5-forecasting-accuracy/'\n\n# Load datasets using the common path\nsales_train_validation = pd.read_csv(f'{data_path}sales_train_validation.csv')\nsales_train_evaluation = pd.read_csv(f'{data_path}sales_train_evaluation.csv')\ncalendar = pd.read_csv(f'{data_path}calendar.csv')\nsell_prices = pd.read_csv(f'{data_path}sell_prices.csv')\nsample_submission = pd.read_csv(f'{data_path}sample_submission.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:33:59.924319Z","iopub.execute_input":"2024-08-10T19:33:59.924706Z","iopub.status.idle":"2024-08-10T19:34:15.784814Z","shell.execute_reply.started":"2024-08-10T19:33:59.924679Z","shell.execute_reply":"2024-08-10T19:34:15.783619Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Melt the sales_train_validation data\nsales_train_validation_melted = sales_train_validation.melt(\n    id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n    var_name='d',\n    value_name='sales'\n)\n\n# Display the first few rows of the melted DataFrame\nprint(\"Melted Sales Train Validation Data:\")\nprint(sales_train_validation_melted.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:34:15.785935Z","iopub.execute_input":"2024-08-10T19:34:15.786364Z","iopub.status.idle":"2024-08-10T19:34:30.192761Z","shell.execute_reply.started":"2024-08-10T19:34:15.786325Z","shell.execute_reply":"2024-08-10T19:34:30.191372Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Melted Sales Train Validation Data:\n                              id        item_id    dept_id   cat_id store_id  \\\n0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n\n  state_id    d  sales  \n0       CA  d_1      0  \n1       CA  d_1      0  \n2       CA  d_1      0  \n3       CA  d_1      0  \n4       CA  d_1      0  \n","output_type":"stream"}]},{"cell_type":"code","source":"def merge_in_chunks(df, calendar, prices, chunk_size=1500000):\n    # List to hold the merged chunks\n    merged_chunks = []\n    \n    # Number of chunks\n    num_chunks = len(df) // chunk_size + 1\n    \n    for i in range(num_chunks):\n        start = i * chunk_size\n        end = (i + 1) * chunk_size\n        chunk = df[start:end]\n        chunk = chunk.merge(calendar, on='d', how='left')\n        chunk = chunk.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n              \n        merged_chunks.append(chunk)\n        \n        print(f\"Chunk {i+1}/{num_chunks} merged\")\n    \n    # Concatenate all chunks\n    merged_df = pd.concat(merged_chunks, ignore_index=True)\n    \n    return merged_df\n\n# Merge the melted DataFrame in chunks\nsales_train_validation_merged = merge_in_chunks(sales_train_validation_melted, calendar, sell_prices)\n\n# Display the first few rows of the merged DataFrame\nprint(\"Merged Sales Train Validation Data:\")\nprint(sales_train_validation_merged.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:34:30.195777Z","iopub.execute_input":"2024-08-10T19:34:30.196400Z","iopub.status.idle":"2024-08-10T19:37:52.511831Z","shell.execute_reply.started":"2024-08-10T19:34:30.196361Z","shell.execute_reply":"2024-08-10T19:37:52.510695Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Chunk 1/39 merged\nChunk 2/39 merged\nChunk 3/39 merged\nChunk 4/39 merged\nChunk 5/39 merged\nChunk 6/39 merged\nChunk 7/39 merged\nChunk 8/39 merged\nChunk 9/39 merged\nChunk 10/39 merged\nChunk 11/39 merged\nChunk 12/39 merged\nChunk 13/39 merged\nChunk 14/39 merged\nChunk 15/39 merged\nChunk 16/39 merged\nChunk 17/39 merged\nChunk 18/39 merged\nChunk 19/39 merged\nChunk 20/39 merged\nChunk 21/39 merged\nChunk 22/39 merged\nChunk 23/39 merged\nChunk 24/39 merged\nChunk 25/39 merged\nChunk 26/39 merged\nChunk 27/39 merged\nChunk 28/39 merged\nChunk 29/39 merged\nChunk 30/39 merged\nChunk 31/39 merged\nChunk 32/39 merged\nChunk 33/39 merged\nChunk 34/39 merged\nChunk 35/39 merged\nChunk 36/39 merged\nChunk 37/39 merged\nChunk 38/39 merged\nChunk 39/39 merged\nMerged Sales Train Validation Data:\n                              id        item_id    dept_id   cat_id store_id  \\\n0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n\n  state_id    d  sales        date  wm_yr_wk  ... month  year  event_name_1  \\\n0       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n1       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n2       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n3       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n4       CA  d_1      0  2011-01-29     11101  ...     1  2011           NaN   \n\n   event_type_1 event_name_2 event_type_2 snap_CA snap_TX  snap_WI  sell_price  \n0           NaN          NaN          NaN       0       0        0         NaN  \n1           NaN          NaN          NaN       0       0        0         NaN  \n2           NaN          NaN          NaN       0       0        0         NaN  \n3           NaN          NaN          NaN       0       0        0         NaN  \n4           NaN          NaN          NaN       0       0        0         NaN  \n\n[5 rows x 22 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fill missing sell prices with the last available price\nsales_train_validation_merged['sell_price'] = sales_train_validation_merged['sell_price'].ffill()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:37:52.513021Z","iopub.execute_input":"2024-08-10T19:37:52.513335Z","iopub.status.idle":"2024-08-10T19:37:53.178099Z","shell.execute_reply.started":"2024-08-10T19:37:52.513308Z","shell.execute_reply":"2024-08-10T19:37:53.176793Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def create_rolling_features(df, window_sizes):\n    for window in window_sizes:\n        df[f'rolling_mean_{window}'] = df.groupby(['id'])['sales'].shift(1).rolling(window=window).mean()\n        df[f'rolling_std_{window}'] = df.groupby(['id'])['sales'].shift(1).rolling(window=window).std()\n    return df\n\n# Apply rolling features\nwindow_sizes = [7, 30]\nsales_train_validation_merged = create_rolling_features(sales_train_validation_merged, window_sizes)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:37:53.181147Z","iopub.execute_input":"2024-08-10T19:37:53.181649Z","iopub.status.idle":"2024-08-10T19:38:55.330182Z","shell.execute_reply.started":"2024-08-10T19:37:53.181617Z","shell.execute_reply":"2024-08-10T19:38:55.328484Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Handle NaNs in rolling features\nrolling_mean_cols = [f'rolling_mean_{window}' for window in window_sizes]\nrolling_std_cols = [f'rolling_std_{window}' for window in window_sizes]\n\n# Fill NaNs in rolling features with the overall mean and std (assuming that initial periods can use overall stats)\nsales_train_validation_merged[rolling_mean_cols] = sales_train_validation_merged[rolling_mean_cols].fillna(sales_train_validation_merged['sales'].mean())\nsales_train_validation_merged[rolling_std_cols] = sales_train_validation_merged[rolling_std_cols].fillna(sales_train_validation_merged['sales'].std())","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:38:55.331742Z","iopub.execute_input":"2024-08-10T19:38:55.332221Z","iopub.status.idle":"2024-08-10T19:38:58.642952Z","shell.execute_reply.started":"2024-08-10T19:38:55.332187Z","shell.execute_reply":"2024-08-10T19:38:58.642049Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Reduce Memory Usage by Downcasting\nHere we convert data types to use less memory. For example, converting float64 to float32 and int64 to int32 can save memory.","metadata":{}},{"cell_type":"code","source":"def reduce_memory_usage(df):\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n        else:\n            df[col] = df[col].astype('category')\n\n    return df\n\nsales_train_validation_merged = reduce_memory_usage(sales_train_validation_merged)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:38:58.644378Z","iopub.execute_input":"2024-08-10T19:38:58.644679Z","iopub.status.idle":"2024-08-10T19:40:04.218935Z","shell.execute_reply.started":"2024-08-10T19:38:58.644653Z","shell.execute_reply":"2024-08-10T19:40:04.217813Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Check dataframe before reduction.\nprint(sales_train_validation_merged.columns)\nprint(sales_train_validation_merged.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:40:04.220305Z","iopub.execute_input":"2024-08-10T19:40:04.220629Z","iopub.status.idle":"2024-08-10T19:40:04.238875Z","shell.execute_reply.started":"2024-08-10T19:40:04.220600Z","shell.execute_reply":"2024-08-10T19:40:04.237898Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n       'sales', 'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'rolling_mean_7',\n       'rolling_std_7', 'rolling_mean_30', 'rolling_std_30'],\n      dtype='object')\n                              id        item_id    dept_id   cat_id store_id  \\\n0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n\n  state_id    d  sales        date  wm_yr_wk  ... event_name_2  event_type_2  \\\n0       CA  d_1      0  2011-01-29     11101  ...          NaN           NaN   \n1       CA  d_1      0  2011-01-29     11101  ...          NaN           NaN   \n2       CA  d_1      0  2011-01-29     11101  ...          NaN           NaN   \n3       CA  d_1      0  2011-01-29     11101  ...          NaN           NaN   \n4       CA  d_1      0  2011-01-29     11101  ...          NaN           NaN   \n\n   snap_CA  snap_TX snap_WI sell_price rolling_mean_7 rolling_std_7  \\\n0        0        0       0        NaN       1.125977      3.873047   \n1        0        0       0        NaN       1.125977      3.873047   \n2        0        0       0        NaN       1.125977      3.873047   \n3        0        0       0        NaN       1.125977      3.873047   \n4        0        0       0        NaN       1.125977      3.873047   \n\n   rolling_mean_30  rolling_std_30  \n0         1.125977        3.873047  \n1         1.125977        3.873047  \n2         1.125977        3.873047  \n3         1.125977        3.873047  \n4         1.125977        3.873047  \n\n[5 rows x 26 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Remove Unnecessary Columns\nDrop any columns that aren't essential for the modeling process.","metadata":{}},{"cell_type":"code","source":"# List of columns to remove\ncolumns_to_remove = [\n    'dept_id', 'cat_id', 'state_id', 'weekday', \n    'wday', 'month', 'year', 'event_type_1', \n    'event_name_2', 'event_type_2'\n]\n\n# Remove the unwanted columns\nsales_train_validation_merged = sales_train_validation_merged.drop(columns=columns_to_remove)\n\n# Binary encoding: 1 if there's any event, 0 otherwise\nsales_train_validation_merged['event_occurred'] = sales_train_validation_merged['event_name_1'].notna().astype(int)\n\n# Drop the original 'event_name_1' column\nsales_train_validation_merged = sales_train_validation_merged.drop('event_name_1', axis=1)\n\n# Verify the final DataFrame\nprint(sales_train_validation_merged.head())\nprint(sales_train_validation_merged.columns)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:40:04.242367Z","iopub.execute_input":"2024-08-10T19:40:04.242688Z","iopub.status.idle":"2024-08-10T19:40:10.852349Z","shell.execute_reply.started":"2024-08-10T19:40:04.242660Z","shell.execute_reply":"2024-08-10T19:40:10.851162Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"                              id        item_id store_id    d  sales  \\\n0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001     CA_1  d_1      0   \n1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002     CA_1  d_1      0   \n2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003     CA_1  d_1      0   \n3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004     CA_1  d_1      0   \n4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005     CA_1  d_1      0   \n\n         date  wm_yr_wk  snap_CA  snap_TX  snap_WI  sell_price  \\\n0  2011-01-29     11101        0        0        0         NaN   \n1  2011-01-29     11101        0        0        0         NaN   \n2  2011-01-29     11101        0        0        0         NaN   \n3  2011-01-29     11101        0        0        0         NaN   \n4  2011-01-29     11101        0        0        0         NaN   \n\n   rolling_mean_7  rolling_std_7  rolling_mean_30  rolling_std_30  \\\n0        1.125977       3.873047         1.125977        3.873047   \n1        1.125977       3.873047         1.125977        3.873047   \n2        1.125977       3.873047         1.125977        3.873047   \n3        1.125977       3.873047         1.125977        3.873047   \n4        1.125977       3.873047         1.125977        3.873047   \n\n   event_occurred  \n0               0  \n1               0  \n2               0  \n3               0  \n4               0  \nIndex(['id', 'item_id', 'store_id', 'd', 'sales', 'date', 'wm_yr_wk',\n       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'rolling_mean_7',\n       'rolling_std_7', 'rolling_mean_30', 'rolling_std_30', 'event_occurred'],\n      dtype='object')\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the features and target\nfeatures = ['sell_price', 'wm_yr_wk', 'snap_CA', 'snap_TX', 'snap_WI', \n            'rolling_mean_7', 'rolling_mean_30', 'rolling_std_7', 'rolling_std_30']\ntarget = 'sales'","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:40:10.853796Z","iopub.execute_input":"2024-08-10T19:40:10.854117Z","iopub.status.idle":"2024-08-10T19:40:10.859244Z","shell.execute_reply.started":"2024-08-10T19:40:10.854090Z","shell.execute_reply":"2024-08-10T19:40:10.858166Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"models = {}  # Dictionary to store models and scalers for each department\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\n# Get unique ids\nid = sales_train_validation_merged['id'].unique()\n\n# Calculate total number of items\ntotal_items = len(id)\n\nfor idx, items in enumerate(id, start=1):\n    # Filter data for the current department\n    items_data = sales_train_validation_merged[sales_train_validation_merged['id'] == items]\n    \n    # Prepare features and target\n    X = items_data[features]\n    y = items_data[target]\n    \n    # Split data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # Standardize the features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Handle NaNs\n    X_train_scaled = pd.DataFrame(X_train_scaled, columns=features)\n    X_train_scaled.fillna(X_train_scaled.mean(), inplace=True)\n    X_val_scaled = pd.DataFrame(X_val_scaled, columns=features)\n    X_val_scaled.fillna(X_val_scaled.mean(), inplace=True)\n    X_train_scaled = X_train_scaled.values\n    X_val_scaled = X_val_scaled.values\n    \n    # Initialize the model\n    model = SGDRegressor(max_iter=1000, tol=1e-3)\n    \n    # Train the model on the entire dataset\n    model.fit(X_train_scaled, y_train)\n    \n    # Store the model and scaler for the current department\n    models[items] = (model, scaler)\n    \n    # Evaluate the model\n    y_pred = model.predict(X_val_scaled)\n    mse = mean_squared_error(y_val, y_pred)\n    \n    # Print progress every 5,000 items\n    if idx % 5000 == 0 or idx == total_items:\n        progress = (idx / total_items) * 100\n        print(f\"Progress: {progress:.2f}% ({idx}/{total_items} items processed)\")\n\nprint(\"All items processed and models stored.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T19:40:10.860648Z","iopub.execute_input":"2024-08-10T19:40:10.861322Z","iopub.status.idle":"2024-08-10T20:11:27.508520Z","shell.execute_reply.started":"2024-08-10T19:40:10.861293Z","shell.execute_reply":"2024-08-10T20:11:27.507126Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Progress: 16.40% (5000/30490 items processed)\nProgress: 32.80% (10000/30490 items processed)\nProgress: 49.20% (15000/30490 items processed)\nProgress: 65.60% (20000/30490 items processed)\nProgress: 81.99% (25000/30490 items processed)\nProgress: 98.39% (30000/30490 items processed)\nProgress: 100.00% (30490/30490 items processed)\nAll items processed and models stored.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load the necessary files\ncalendar = pd.read_csv(f'{data_path}calendar.csv')\nsell_prices = pd.read_csv(f'{data_path}sell_prices.csv')\nsubmission_df = pd.read_csv(f'{data_path}sample_submission.csv')\n\n# Convert 'date' column in calendar to datetime\ncalendar['date'] = pd.to_datetime(calendar['date'])\n\n# Generate dates for the next 28 days\nstart_date = '2016-04-25'\nnext_28_days_dates = pd.date_range(start=start_date, periods=28)\nnext_28_days = pd.DataFrame({'date': next_28_days_dates})\n\n# Merge with calendar data to get required features\nnext_28_days = next_28_days.merge(calendar, how='left', on='date')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-10T20:11:27.510332Z","iopub.execute_input":"2024-08-10T20:11:27.510748Z","iopub.status.idle":"2024-08-10T20:11:31.367411Z","shell.execute_reply.started":"2024-08-10T20:11:27.510710Z","shell.execute_reply":"2024-08-10T20:11:31.366361Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# List of columns to remove\ncolumns_to_remove = [\n    'weekday', 'wday', 'month', 'year', 'event_type_1', 'event_name_2', 'event_type_2'\n]\n# Remove the unwanted columns\nnext_28_days = next_28_days.drop(columns=columns_to_remove)\n\n# Binary encoding: 1 if there's any event, 0 otherwise\nnext_28_days['event_occurred'] = next_28_days['event_name_1'].notna().astype(int)\n\n# Drop the original 'event_name_1' column\nnext_28_days = next_28_days.drop('event_name_1', axis=1)\n\nunique_ids = sales_train_validation_merged['id'].unique()\n\nnext_28_days['id'] = unique_ids[:len(next_28_days)]","metadata":{"execution":{"iopub.status.busy":"2024-08-10T20:11:31.368879Z","iopub.execute_input":"2024-08-10T20:11:31.369237Z","iopub.status.idle":"2024-08-10T20:11:31.652928Z","shell.execute_reply.started":"2024-08-10T20:11:31.369205Z","shell.execute_reply":"2024-08-10T20:11:31.652030Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Bring back the sell_price to the 28 day data set because we reckon it as an important feature.","metadata":{}},{"cell_type":"code","source":"# Merge to add 'item_id' from sales_train_validation_merged to next_28_days\nnext_28_days = next_28_days.merge(sales_train_validation_merged[['id', 'item_id']], how='left', on='id')\nnext_28_days = next_28_days.merge(sell_prices[['item_id', 'wm_yr_wk', 'sell_price']], how='left', on=['item_id', 'wm_yr_wk'])\n\n# Verify the merge\nprint(next_28_days.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-10T20:11:31.654201Z","iopub.execute_input":"2024-08-10T20:11:31.654595Z","iopub.status.idle":"2024-08-10T20:11:36.710792Z","shell.execute_reply.started":"2024-08-10T20:11:31.654560Z","shell.execute_reply":"2024-08-10T20:11:36.709695Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"        date  wm_yr_wk       d  snap_CA  snap_TX  snap_WI  event_occurred  \\\n0 2016-04-25     11613  d_1914        0        0        0               0   \n1 2016-04-25     11613  d_1914        0        0        0               0   \n2 2016-04-25     11613  d_1914        0        0        0               0   \n3 2016-04-25     11613  d_1914        0        0        0               0   \n4 2016-04-25     11613  d_1914        0        0        0               0   \n\n                              id        item_id  sell_price  \n0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001        8.38  \n1  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001        8.38  \n2  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001        8.38  \n3  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001        8.38  \n4  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001        8.26  \n","output_type":"stream"}]}]}